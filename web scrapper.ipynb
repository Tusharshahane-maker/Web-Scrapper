{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c20035-4ec0-4584-b84a-35efd0b1455f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Akash'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dfdca9-7c38-4660-8640-04cce5121ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 books:\n",
      "1. A Light in the ...\n",
      "2. Tipping the Velvet\n",
      "3. Soumission\n",
      "4. Sharp Objects\n",
      "5. Sapiens: A Brief History ...\n",
      "6. The Requiem Red\n",
      "7. The Dirty Little Secrets ...\n",
      "8. The Coming Woman: A ...\n",
      "9. The Boys in the ...\n",
      "10. The Black Maria\n",
      "11. Starving Hearts (Triangular Trade ...\n",
      "12. Shakespeare's Sonnets\n",
      "13. Set Me Free\n",
      "14. Scott Pilgrim's Precious Little ...\n",
      "15. Rip it Up and ...\n",
      "16. Our Band Could Be ...\n",
      "17. Olio\n",
      "18. Mesaerion: The Best Science ...\n",
      "19. Libertarianism for Beginners\n",
      "20. It's Only the Himalayas\n",
      "\n",
      "Data saved to book_titles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_website(url, selector):\n",
    "    \"\"\"\n",
    "    Basic web scraper function\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): The URL to scrape\n",
    "    selector (str): CSS selector to target specific elements\n",
    "    \n",
    "    Returns:\n",
    "    list: Extracted data\n",
    "    \"\"\"\n",
    "    # Add headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Make the request\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract data using the provided selector\n",
    "    elements = soup.select(selector)\n",
    "    \n",
    "    # Extract text from elements\n",
    "    results = [element.text.strip() for element in elements]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: Scraping book titles from books.toscrape.com\n",
    "    url = \"http://books.toscrape.com/\"\n",
    "    selector = \"article.product_pod h3 a\"\n",
    "    \n",
    "    # Scrape the data\n",
    "    book_titles = scrape_website(url, selector)\n",
    "    \n",
    "    # Display results\n",
    "    if book_titles:\n",
    "        print(f\"Found {len(book_titles)} books:\")\n",
    "        for i, title in enumerate(book_titles, 1):\n",
    "            print(f\"{i}. {title}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(book_titles, columns=[\"Book Title\"])\n",
    "        df.to_csv(\"book_titles.csv\", index=False)\n",
    "        print(\"\\nData saved to book_titles.csv\")\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec3f5b5-9865-43c4-a13c-925297db61dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Akash\n",
      "\n",
      "Files in current directory:\n",
      " - book_titles.csv\n",
      "\n",
      "Pandas version: 2.2.3\n",
      "Data files would typically be read from: C:\\Users\\Akash\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# List files in the current directory\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for file in os.listdir():\n",
    "    # Only show data files with common extensions\n",
    "    if file.endswith(('.csv', '.xlsx', '.json', '.pkl', '.h5', '.parquet', '.db', '.sqlite')):\n",
    "        print(f\" - {file}\")\n",
    "\n",
    "# If you're using pandas, you might want to check its version instead\n",
    "import pandas as pd\n",
    "print(f\"\\nPandas version: {pd.__version__}\")\n",
    "\n",
    "# If you need to know where pandas might look for data, you can print the current directory\n",
    "print(f\"Data files would typically be read from: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "349d8f96-80f5-4def-87c4-bbe8287a49b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Akash\n",
      "\n",
      "Files in current directory:\n",
      " - book_titles.csv\n",
      "\n",
      "Pandas version: 2.2.3\n",
      "Data files would typically be read from: C:\\Users\\Akash\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# List files in the current directory\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for file in os.listdir():\n",
    "    # Only show data files with common extensions\n",
    "    if file.endswith(('.csv', '.xlsx', '.json', '.pkl', '.h5', '.parquet', '.db', '.sqlite')):\n",
    "        print(f\" - {file}\")\n",
    "\n",
    "# If you're using pandas, you might want to check its version instead\n",
    "import pandas as pd\n",
    "print(f\"\\nPandas version: {pd.__version__}\")\n",
    "\n",
    "# If you need to know where pandas might look for data, you can print the working directory\n",
    "print(f\"Data files would typically be read from: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25390f02-c3a9-4450-b03e-479122ca7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Beautiful Soup using pip\n",
    "!pip install beautifulsoup4\n",
    "\n",
    "# To verify the installation and show how to import it\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "print(f\"Beautiful Soup version: {bs4.__version__}\")\n",
    "print(\"Beautiful Soup installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824244f-b064-4282-b94e-6551e2284e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Install required libraries if not already installed\n",
    "try:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    !pip install requests beautifulsoup4\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape a website\n",
    "def scrape_website(url):\n",
    "    # Send a request to the website\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for 4XX/5XX responses\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Example: Extract all headlines (h1, h2, h3 tags)\n",
    "        headlines = []\n",
    "        for i in range(1, 4):\n",
    "            for heading in soup.find_all(f'h{i}'):\n",
    "                headlines.append({\n",
    "                    'level': i,\n",
    "                    'text': heading.text.strip()\n",
    "                })\n",
    "        \n",
    "        # Example: Extract all links\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            links.append({\n",
    "                'text': link.text.strip(),\n",
    "                'url': link['href']\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'title': soup.title.text if soup.title else 'No title found',\n",
    "            'headlines': headlines,\n",
    "            'links': links[:10]  # Limiting to first 10 links for brevity\n",
    "        }\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'error': f\"Request error: {e}\"}\n",
    "    except Exception as e:\n",
    "        return {'error': f\"Error: {e}\"}\n",
    "\n",
    "# Example usage\n",
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"  # Example URL\n",
    "print(\"Scraping website...\")\n",
    "result = scrape_website(url)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nTitle: {result.get('title')}\")\n",
    "\n",
    "print(\"\\nSample Headlines:\")\n",
    "for headline in result.get('headlines', [])[:5]:  # Show first 5 headlines\n",
    "    print(f\"H{headline['level']}: {headline['text']}\")\n",
    "\n",
    "print(\"\\nSample Links:\")\n",
    "for link in result.get('links', [])[:5]:  # Show first 5 links\n",
    "    print(f\"- {link['text']}: {link['url']}\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "headlines_df = pd.DataFrame(result.get('headlines', []))\n",
    "links_df = pd.DataFrame(result.get('links', []))\n",
    "\n",
    "print(\"\\nHeadlines DataFrame Preview:\")\n",
    "if not headlines_df.empty:\n",
    "    print(headlines_df.head())\n",
    "\n",
    "print(\"\\nLinks DataFrame Preview:\")\n",
    "if not links_df.empty:\n",
    "    print(links_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c16eb17-51ec-48ee-a83b-dc23a848f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://en.wikipedia.org/wiki/Python_(programming_language)...\n",
      "\n",
      "Title: Python (programming language) - Wikipedia\n",
      "\n",
      "First few paragraphs:\n",
      "2. Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code ...\n",
      "3. Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming lan...\n",
      "4. Python has gained widespread use in the machine learning community.[36][37][38][39] It is widely tau...\n",
      "5. Python was conceived in the late 1980s[42] by Guido van Rossum at Centrum Wiskunde & Informatica (CW...\n",
      "\n",
      "Some links found:\n",
      "1. Jump to content -> #bodyContent\n",
      "2. Main page -> /wiki/Main_Page\n",
      "3. Contents -> /wiki/Wikipedia:Contents\n",
      "4. Current events -> /wiki/Portal:Current_events\n",
      "5. Random article -> /wiki/Special:Random\n",
      "6. About Wikipedia -> /wiki/Wikipedia:About\n",
      "7. Contact us -> //en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "8. Help -> /wiki/Help:Contents\n",
      "9. Learn to edit -> /wiki/Help:Introduction\n",
      "10. Community portal -> /wiki/Wikipedia:Community_portal\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Install required libraries if not already installed\n",
    "try:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    !pip install requests beautifulsoup4\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape a website\n",
    "def scrape_website(url):\n",
    "    # Send a request to the website\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get the webpage content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Get the page title\n",
    "        title = soup.title.text if soup.title else \"No title found\"\n",
    "        \n",
    "        # Get all paragraph text\n",
    "        paragraphs = [p.text.strip() for p in soup.find_all('p')]\n",
    "        \n",
    "        # Get all links\n",
    "        links = [{'text': a.text.strip(), 'url': a['href']} \n",
    "                for a in soup.find_all('a', href=True) if a.text.strip()]\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'paragraphs': paragraphs[:5],  # First 5 paragraphs\n",
    "            'links': links[:10]  # First 10 links\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Example usage\n",
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "print(f\"Scraping {url}...\")\n",
    "\n",
    "# Get the data\n",
    "data = scrape_website(url)\n",
    "\n",
    "# Print the results\n",
    "if 'error' in data:\n",
    "    print(f\"Error: {data['error']}\")\n",
    "else:\n",
    "    print(f\"\\nTitle: {data['title']}\")\n",
    "    \n",
    "    print(\"\\nFirst few paragraphs:\")\n",
    "    for i, p in enumerate(data['paragraphs'], 1):\n",
    "        if p:  # Only print non-empty paragraphs\n",
    "            print(f\"{i}. {p[:100]}...\")  # Print first 100 chars of each paragraph\n",
    "    \n",
    "    print(\"\\nSome links found:\")\n",
    "    for i, link in enumerate(data['links'], 1):\n",
    "        print(f\"{i}. {link['text']} -> {link['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "578e0866-ef09-4ec3-ac6e-2de065360d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from https://www.example.com...\n",
      "\n",
      "Page Title: Example Domain\n",
      "\n",
      "Headings:\n",
      "- Example Domain\n",
      "\n",
      "Paragraphs:\n",
      "- This domain is for use in documentation examples without needing permission. Avoid use in operations...\n",
      "- Learn more...\n",
      "\n",
      "Links:\n",
      "- Learn more: https://iana.org/domains/example\n",
      "\n",
      "Web scraping completed!\n"
     ]
    }
   ],
   "source": [
    "# Simple web scraper using Beautiful Soup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Install required packages if not already installed\n",
    "try:\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    !pip install requests beautifulsoup4\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.example.com\"\n",
    "\n",
    "# Send HTTP request to the URL\n",
    "print(f\"Fetching content from {url}...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract and print the title\n",
    "    title = soup.title.string\n",
    "    print(f\"\\nPage Title: {title}\")\n",
    "    \n",
    "    # Extract and print all headings\n",
    "    print(\"\\nHeadings:\")\n",
    "    for heading in soup.find_all(['h1', 'h2', 'h3']):\n",
    "        print(f\"- {heading.text.strip()}\")\n",
    "    \n",
    "    # Extract and print all paragraphs\n",
    "    print(\"\\nParagraphs:\")\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        print(f\"- {paragraph.text.strip()[:100]}...\")  # Print first 100 chars\n",
    "    \n",
    "    # Extract and print all links\n",
    "    print(\"\\nLinks:\")\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        print(f\"- {link.text.strip() or '[No text]'}: {link['href']}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "print(\"\\nWeb scraping completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
